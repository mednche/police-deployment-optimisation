# Determining optimal police patrol deployments: a simulation-based optimisation approach combining Agent-Based Modelling and Genetic Algorithms.


## Project Description

This repository provides complementary code and data for the work undertaken during my PhD.




### Motivations: 

Police agencies face the challenge of deploying patrols effectively to both (1) respond to calls and (2) deter crime through patrolling, all the while keeping operational costs low. These are conflicting objectives of efficiency and effectiveness related to the proactive and the reactive demand that they face.

This repo contains a decision-support tool which can identify deployments that offer tradeoffs between these objectives. The tool was built so as to be applied to any police force. It is made of two components:
- an Agent-Based Model that simulates the dispatching of police patrols throughout a given shift
- a Genetic Algorithm that searches for the optimal number and spatial positioning of police patrols.

<!---
- Why you used the technologies you used,
- Some of the challenges you faced and features you hope to implement in the future.
-->

In what follows, two types of shifts are studied separately so that the tool can prescribe a solution that is tailored to the demand faced:
-	low-demand: weekdays (8am-4pm)
- high-demand: weekend days (midnight-8am)


## Structure of the repo

The `src` folder contains the model codebase, split into an `ABM` folder - containing the code for running the ABM on its own - and a `GA` folder with the code to run the ABM-based (single and multi-objective) optimisation.

The `data` folder is where all the necessary data files need to be placed to run the analysis. Ensure the files are named correctly when you replace them with your own. The required files are:

1. Spatial entities:
- `patrol_beats` folder: contains the shapefiles for the patrol beats (i.e. the smaller spatial unit within which agents are deployed to patrol)
- `precincts` folder: contains the shapefiles for the districts/precincts (i.e. the larger spatial unit within which agents are dispatched to calls)
- `G.gpickle`: the networkX graph representing the road network of the police force (see below for details on how it is generated)
- `G_proj.gpickle`: the networkX graph representing the road network of the police force, projected to UTM (see below for details on how it is generated)

2. Historical crimes and CFS incidents:
- `incidents.csv`: a dataset of historical CFS incidents (see below for details of required columns)
- `crimes.csv`: a dataset of historical crimes (see below for details of required columns) 

3. Historical, training and test sets for ABM experiments and GA training (see below for how these are generated):
- `historical_set_scenario1.pkl`
- `historical_set_scenario2.pkl`
- `training_set_scenario1.pkl`
- `training_set_scenario2.pkl`
- `testing_set_scenario1.pkl`
- `testing_set_scenario2.pkl`

The folder `dpd_case_study` contains all the files pertaining to the case study on Detroit Police Department (DPD), Michigan folders and jupyter pyton scripts and notebooks to run the ABM and the GAs.


## Installation

Key dependencies include (see provided yml file for versions that were used):

For the ABM:
- `osmnx` for downlading the road network of a police force and calculating fastest routes
- `networkX`for manipulating the road network graph generated by osmnx
- `geopandas` for manipulating the spatial dataframes generated by osmnx
- `imageio` for producing a GIF of the ABM
- `multiprocessing` (optional) to use multiple cores and thus speed up the running of the ABM

For the GA:
- `deap`

The `ABM` codebase is composed of the following files:
- `AgentFramework.py`: the attributes and methods for the agent entities
- `ModelFramework.py`: the model entity
- `SchedulerFramework.py`: the model scheduler 
- `IncidentFramework.py`: the attributes and methods for the incident entities
- `GraphFramework.py`: the attributes and methods for the road network (networkX graph and complimentary node and edge data)
- `BeatFramework.py`: the attributes and methods for the patrol beats
- `Env.py`: the initialisation of the model environment composed of the road network, the patrol beats and the incidents for the time period.



## Usage (example of Detroit)

#### Aquiring data

To run the ABM, it is necessary to first aquire the following files and place them in the `data` folder: 

1. `patrol_beats.shp`: Aquire the patrol beat shapefile.
Columns:
  - name
  - precinct
  - geometry 
3. `precincts.shp`: Aquire the precinct shapefile.
Columns:
  - name
  - geometry
5. `incidents.csv`: Aquire a dataset of historical CFS incidents.
Columns:
  - Date_Time
  - lat
  - lon
  - Precinct
  - Patrol_beat
  - Time On Scene
5. `crimes.csv`: Aquire a dataset of historical crimes
Columns:
  - Date_Time
  - lat
  - lon
  - Patrol_beat


#### Preprocessing

Next, run the jupyter notebook in `dpd_case_study/data_preprocessing.ipynb` to preprocess the data prior to running any model. There are 3 preprocessing steps in this notebook:
1. Download the road network for your chosen police force, using `osmnx`, preprocess the graph to add some columns and save it as `G.pickle` and its projected version as `G_proj.gpickle`.
2. Preprocess the `patrol_beats.shp` to add the `centroid_node` column
3. Preprocess the `incidents.csv` and `crimes.csv` to add a `Node` column to the datasets. This is not done in the notebook itself but need to be run in command line using multiprocessingdue to the size of the dataset.

#### Generating historical, training and test sets

In `dpd_case_study/generating_data_sets.ipynb` we create historical, training and test sets for both demand scenarios. 

A set is a collection of 100 specifically selected time periods from sepearate years to mimic the manner in which police agencies make decisions based on past demand.
- historical set: 100 time periods from 2017
- training set: 100 time periods from 2018
- test set: 100 time periods from 2019

For more details on what these sets are used for, see `dpd_case_study/generating_data_sets.ipynb`.
Ensure that the 6 generated sets are placed in the `data` folder at the source of the repository.

#### Running the ABM

An example on how to run the ABM is provided in `dpd_case_study/ABM_running_example/ABM_running_example`.

![alt text](https://github.com/mednche/police-deployment-optimisation/blob/main/dpd_case_study/ABM_running_example/ABM_animation_gif/20_agents/20_steps.gif)

For faster results, it is recommended to harness the power of multiprocessing if your machine has multiple cores (example not provided). 


#### Running the ABM+GA for DPD


Expected time of running the GA on multiprocessing 


## Licence

MIT

